{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvB+8BUlhOYIpwjn1rsdce"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm\n","import ankh\n","import torch\n","from transformers import T5EncoderModel, AutoTokenizer, AutoModel, BertTokenizer, T5Tokenizer\n","device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n","tokenizer = AutoTokenizer.from_pretrained(\"\") # replace this to your file path\n","model = T5EncoderModel.from_pretrained(\"\").to(device)# replace this to your file path\n","\n","def ESM2(file):\n","    with open(file, 'r') as file:\n","        lines = file.readlines()\n","    for i in tqdm(range(0, len(lines), 2)):\n","        protein_id = lines[i].split(\" \")[0]\n","        sequence = lines[i].split(\" \")[1]\n","        outputs = tokenizer(sequence, return_tensors=\"pt\", truncation=True)\n","        with torch.no_grad():\n","            embeddings = model(input_ids=outputs['input_ids'].to(device), attention_mask=outputs['attention_mask'].to(device))\n","        exact_embedding = embeddings[0][0][1:-1].detach().detach().cpu().numpy()\n","        save_path_new = \"\" + protein_id + \".tensor\" # replace this to your file path\n","        torch.save(exact_embedding, save_path_new)\n","\n","\n","data_paths = [\"your file name\"]\n","for data_path in data_paths:\n","    print(data_path)\n","    read_path = \"\" + data_path + \".txt\" # replace this to your file path\n","    ESM2(read_path)"],"metadata":{"id":"3gCCCOu6vNM1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import os\n","import torch,esm\n","import re,sys\n","from tqdm import tqdm\n","from transformers import T5Tokenizer, T5EncoderModel, BertGenerationEncoder, BertTokenizer\n","from sklearn.preprocessing import MinMaxScaler\n","transformer_link = \"\" # your file path\n","tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False)\n","model = T5EncoderModel.from_pretrained(transformer_link)\n","device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model = model.eval()\n","if device==torch.device(\"cuda\"):\n","  model = model.half()\n","\n","\n","def read_sequence(file):\n","    sequence_dict = dict()\n","    label_dict = dict()\n","    with open(file, 'r') as file:\n","        lines = file.readlines()\n","\n","    for i in tqdm(range(0, len(lines), 2)):\n","        protein_id = lines[i].split(\" \")[0]\n","        sequence = lines[i].split(\" \")[1]\n","        lenn = len(sequence)\n","        seq = \"\"\n","        for i in range(lenn):\n","            seq = seq + sequence[i] + \" \"\n","        sequence_dict[protein_id] = seq\n","    return sequence_dict\n","\n","\n","\n","def embed_dataset(seq, shift_left = 0, shirf_right = -1):\n","    with torch.no_grad():\n","        ids = tokenizer.batch_encode_plus([seq], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n","        embedding = model(input_ids = ids['input_ids'].to(device))[0]\n","        embedding = embedding[0][shift_left:shirf_right].detach().detach().cpu().numpy()\n","    return embedding\n","\n","data_paths = [\"your file name\"]\n","for data_path in data_paths:\n","    print(data_path + \"is processing\")\n","    read_path = \"\" + data_path + \".txt\"\n","    feature_dir = \"\"\n","    sequence_dict = read_sequence(read_path)\n","    for protein_id in tqdm(sequence_dict):\n","        seq = sequence_dict[protein_id]\n","        sample = list(seq)\n","        embedding = embed_dataset(sample, shift_left=0, shirf_right=-1)\n","        save_path_new = feature_dir + protein_id + \".tensor\"\n","        torch.save(embedding, save_path_new)\n","        torch.cuda.empty_cache()\n",""],"metadata":{"id":"UxHUYynKwWFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpE2rdy-BRbk"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from model import *\n","import torch.utils.data as data\n","from torch.nn.utils.rnn import pad_sequence\n","class Getdata(data.Dataset):\n","    def __init__(self, ID_list):\n","        super(Getdata, self).__init__()\n","        self.IDs = ID_list\n","\n","    def __len__(self):\n","        return len(self.IDs)\n","\n","    def __getitem__(self, idx):\n","        return self._feature_(idx)\n","\n","    def _feature_(self, idx):\n","        name = self.IDs[idx]\n","        with torch.no_grad():\n","            embedding1 = torch.load(\"../prostt5_embedding/\" + name + \".tensor\")\n","            embedding2 = torch.load(\"../ankh_embedding/\" + name + \".tensor\")\n","            labels = torch.load(\"../label_embedding/\" + name + \".tensor\")\n","        return embedding2, embedding1, labels\n","\n","\n","class BatchCollate(object):\n","    def __call__(self, batch):\n","        features1, features2, labels = zip(*batch)\n","        # 填充特征数据\n","        features1 = [torch.tensor(feature) for feature in features1]\n","        feature1 = pad_sequence(features1, batch_first=True)  # ([bz, length, dim])\n","        features2 = [torch.tensor(feature) for feature in features2]\n","        feature2 = pad_sequence(features2, batch_first=True)  # ([bz, length, dim])\n","        # 填充标签数据\n","        labels = [torch.tensor(label) for label in labels]\n","        label = pad_sequence(labels, batch_first=True, padding_value=-1)  # [bz, length]\n","        return feature1, feature2, label\n","\n","# 设置设备\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 加载测试集蛋白质ID\n","train_text = \"\" # your file path\n","protein_ids = []\n","with open(train_text, 'r') as file:\n","    lines = file.readlines()\n","    for i in range(0, len(lines), 2):\n","        protein_id = lines[i].split(\" \")[0]\n","        protein_ids.append(protein_id)\n","\n","# 加载数据集\n","test_dataset = Getdata(ID_list=protein_ids)\n","test_loader = DataLoader(test_dataset, collate_fn=BatchCollate(), shuffle=False, num_workers=8)  # shuffle改为False以保持顺序\n","\n","# 加载模型\n","best_model = Plan_Ankh_ProstT5()\n","if torch.cuda.device_count() > 1:\n","    best_model = nn.DataParallel(best_model)\n","best_model.eval()\n","best_model.to(device)\n","best_model.load_state_dict(torch.load(f\"/549_Ankh_ProstT5.pt\"))\n","\n","# 存储结果\n","results = []\n","\n","# 模型预测并输出\n","with torch.no_grad():\n","    for protein_id, (data1, data2, label) in zip(protein_ids, test_loader):\n","        data1 = data1.to(device)\n","        data2 = data2.to(device)\n","        label = label.to(device)\n","\n","        # 模型输出\n","        score, _, _, _, _, _ = best_model(data1, data2)\n","        label_list = [out.cpu().numpy() for out in label]\n","        score_list = [out.detach().cpu().numpy() for out in score]\n","        score = np.concatenate(score_list)\n","        label = np.concatenate(label_list)\n","        # 转换数据格式\n","        probabilities = score[:, 1]  # 正样本的概率\n","        predicted_classes = np.argmax(score, axis=1)  # 预测类别\n","        true_labels = label  # 真实标签\n","\n","        # 逐位点存储结果\n","        for i in range(len(probabilities)):\n","            results.append({\n","                \"Protein_ID\": protein_id,\n","                \"Residue_Index\": i + 1,\n","                \"Predicted_Probability\": probabilities[i],\n","                \"Predicted_Class\": predicted_classes[i],\n","                \"True_Label\": true_labels[i]\n","            })\n","\n","# 保存结果到CSV\n","output_df = pd.DataFrame(results)\n","output_df.to_csv(\"result.csv\", index=False)\n","print(\"Results saved to result.csv.\")\n"]}]}